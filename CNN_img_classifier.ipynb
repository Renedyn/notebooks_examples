{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:05.777669Z",
     "iopub.status.busy": "2025-01-19T13:13:05.777361Z",
     "iopub.status.idle": "2025-01-19T13:13:15.758989Z",
     "shell.execute_reply": "2025-01-19T13:13:15.758031Z",
     "shell.execute_reply.started": "2025-01-19T13:13:05.777644Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "Using the GPU 😊\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "!wandb login \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using the GPU 😊\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using the CPU 😞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:17.756346Z",
     "iopub.status.busy": "2025-01-19T13:13:17.756071Z",
     "iopub.status.idle": "2025-01-19T13:13:17.765528Z",
     "shell.execute_reply": "2025-01-19T13:13:17.764663Z",
     "shell.execute_reply.started": "2025-01-19T13:13:17.756326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NETWORK_SIZE = (40, 40)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "LABELS_CNT = 200\n",
    "DEFAULT_TRANSFORM = T.Compose(\n",
    "    [\n",
    "        T.ToImage(),\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Resize(size=NETWORK_SIZE),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "DEFAULT_TRANSFORM_AUG = T.Compose(\n",
    "    [\n",
    "    T.ToImage(),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomResizedCrop(size=NETWORK_SIZE, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
    "        T.TrivialAugmentWide(),\n",
    "        T.RandAugment(),\n",
    "        T.RandomErasing(),\n",
    "        #T.RandomChoice([\n",
    "            #T.AutoAugment(),\n",
    "        #]),\n",
    "        #RandomErasing\n",
    "        #TrivialAugmentWide\n",
    "        #Elastic\n",
    "        #Affine\n",
    "        T.ToDtype(torch.float32, scale=True),\n",
    "        T.Resize(size=NETWORK_SIZE),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt : dict, mode, train_fraction=0.9, rnd_seed=42):\n",
    "        img_paths = glob.glob(f'{img_dir}/*')\n",
    "        labels = np.array([gt[img_path.split('/')[-1]] for img_path in img_paths])\n",
    "\n",
    "        X_train, X_valid = train_test_split(img_paths, train_size=train_fraction, random_state=rnd_seed, stratify=labels)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self._paths = X_train\n",
    "        elif mode == 'valid':\n",
    "            self._paths = X_valid\n",
    "        else:\n",
    "            raise RuntimeError(f\"Invalid mode: {mode!r}\")\n",
    "\n",
    "        self._len = len(self._paths)\n",
    "        self._gt = gt\n",
    "        self._mode = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self._paths[index]\n",
    "        filename = img_path.split('/')[-1]\n",
    "        \n",
    "        image = torchvision.io.read_image(img_path)\n",
    "\n",
    "        if self._mode == 'train':\n",
    "            image = DEFAULT_TRANSFORM_AUG(image)\n",
    "        else:\n",
    "            image = DEFAULT_TRANSFORM(image)\n",
    "        return image, self._gt[filename]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:20.701067Z",
     "iopub.status.busy": "2025-01-19T13:13:20.700785Z",
     "iopub.status.idle": "2025-01-19T13:13:20.789830Z",
     "shell.execute_reply": "2025-01-19T13:13:20.788914Z",
     "shell.execute_reply.started": "2025-01-19T13:13:20.701046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    res = {}\n",
    "    with open(filename) as fhandle:\n",
    "        next(fhandle)\n",
    "        for line in fhandle:\n",
    "            parts = line.rstrip('\\n').split(',')\n",
    "            label = int(parts[1])\n",
    "            res[parts[0]] = label\n",
    "    return res\n",
    "\n",
    "train_gt = read_csv('/kaggle/input/bhw-1-dl-2024-2025/bhw1/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:23.599069Z",
     "iopub.status.busy": "2025-01-19T13:13:23.598714Z",
     "iopub.status.idle": "2025-01-19T13:13:23.618282Z",
     "shell.execute_reply": "2025-01-19T13:13:23.617259Z",
     "shell.execute_reply.started": "2025-01-19T13:13:23.599043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_detector(train_gt : dict, train_img_dir : str, run_name=None,\n",
    "                   epoch_cnt=10, lr=0.01, cos_restart=10, \n",
    "                   batch_size=32, weight_decay=5e-4, sgd_momentum=0.9,\n",
    "                   chan_mult=2, layers=[4, 4, 4], dropout=[0.15, 0.25, 0.35, 0.60],\n",
    "                   model_state=None):\n",
    "    wandb.init(name=run_name, project=\"DL LHW 1\")\n",
    "    \n",
    "    config = wandb.config          # Initialize config\n",
    "    config.batch_size = batch_size          # input batch size for training (default: 64)\n",
    "    config.epochs = epoch_cnt             # number of epochs to train (default: 10)\n",
    "    config.lr = lr               # learning rate (default: 0.01)\n",
    "    config.cos_restart = cos_restart               # learning rate (default: 0.01)\n",
    "    config.momentum = sgd_momentum          # SGD momentum (default: 0.5)\n",
    "    config.weight_decay = weight_decay\n",
    "    config.chan_mult = chan_mult\n",
    "    config.layers = layers\n",
    "    config.dropout = dropout\n",
    "\n",
    "    ds_train = ImgDataset(train_img_dir, train_gt, mode=\"train\", train_fraction=0.9)\n",
    "    ds_valid = ImgDataset(train_img_dir, train_gt, mode=\"valid\", train_fraction=0.9)\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=os.cpu_count()-1, pin_memory=True)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=os.cpu_count()-1, pin_memory=True)\n",
    "\n",
    "    model = ClfModel(chan_mult, layers, dropout).to(DEVICE)\n",
    "    model = nn.DataParallel(model)\n",
    "    if model_state is not None:\n",
    "        model.load_state_dict(model_state)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=sgd_momentum, lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, cos_restart)\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(epoch_cnt):\n",
    "        progress_train = tqdm(\n",
    "            total=len(dl_train),\n",
    "            desc=f\"Epoch {epoch}\",\n",
    "            leave=False,\n",
    "        )\n",
    "        \n",
    "        train_loss_ls = []\n",
    "        train_acc = 0\n",
    "        model = model.train()\n",
    "        batch_ind = 0\n",
    "        train_img_cnt = len(ds_train) // batch_size * batch_size\n",
    "        for x_batch, y_batch in dl_train:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            probs = model(x_batch)\n",
    "            loss = loss_fn(probs, y_batch)  # Calc loss\n",
    "            loss.backward()\n",
    "            optimizer.step() # Backward\n",
    "            scheduler.step(epoch + batch_ind/train_img_cnt)\n",
    "            \n",
    "            train_loss_ls.append(loss.detach())\n",
    "            train_acc += (probs.detach().argmax(axis=1) == y_batch).sum()\n",
    "            progress_train.update()\n",
    "            batch_ind += 1\n",
    "        train_acc = 100*train_acc/train_img_cnt\n",
    "        print(\n",
    "            f\"Epoch {epoch},\",\n",
    "            f\"train_loss: {torch.stack(train_loss_ls).mean().item():.8f}\",\n",
    "            f\"train_accuracy: {train_acc:.8f}\",\n",
    "        )\n",
    "\n",
    "        progress_train.close()\n",
    "\n",
    "        progress_valid = tqdm(\n",
    "            total=len(dl_valid),\n",
    "            desc=f\"Epoch {epoch}\",\n",
    "            leave=False,\n",
    "        )\n",
    "        model = model.eval()\n",
    "        valid_loss_ls = []\n",
    "        valid_acc = 0\n",
    "        for x_batch, y_batch in dl_valid:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                probs = model(x_batch)\n",
    "                loss = loss_fn(probs, y_batch)  # Calc loss\n",
    "            valid_loss_ls.append(loss.detach())\n",
    "            valid_acc += (probs.detach().argmax(axis=1) == y_batch).sum()\n",
    "            progress_valid.update()\n",
    "                    \n",
    "        progress_valid.close()\n",
    "\n",
    "        val_loss = torch.stack(valid_loss_ls).mean().item()\n",
    "        valid_acc = (100*valid_acc/len(ds_valid)).item()\n",
    "        print(\n",
    "            f\"Epoch {epoch},\",\n",
    "            f\"valid_loss: {val_loss:.8f}\",\n",
    "            f\"valid_accuracy: {valid_acc:.8f}\",\n",
    "        )\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'model_checkpoint_{run_name}_e{epoch}.pt')\n",
    "            wandb.save(f'/kaggle/working/model_checkpoint_{run_name}_e{epoch}.pt')\n",
    "        if best_val_loss > val_loss and val_loss < 2.1:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "            wandb.save('model.pt')\n",
    "        print(scheduler.get_last_lr())\n",
    "        wandb.log({\n",
    "            \"Learning rate\" : scheduler.get_last_lr()[-1],\n",
    "            \"Train Accuracy\": train_acc.item(),\n",
    "            \"Train Loss\": torch.stack(train_loss_ls).mean().item(),\n",
    "            \"Valid Accuracy\": valid_acc,\n",
    "            \"Valid Loss\": val_loss\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:26.847113Z",
     "iopub.status.busy": "2025-01-19T13:13:26.846823Z",
     "iopub.status.idle": "2025-01-19T13:13:26.857651Z",
     "shell.execute_reply": "2025-01-19T13:13:26.856810Z",
     "shell.execute_reply.started": "2025-01-19T13:13:26.847092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, layers=4, dropout=0.0, kernel=3, reduce=True) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(layers):\n",
    "            self.convs.append(nn.Conv2d(in_c if i == 0 else out_c, out_c, kernel, padding='same', bias=False))\n",
    "            self.bns.append(nn.BatchNorm2d(out_c, momentum=0.1))\n",
    "        self.convs = nn.Sequential(*self.convs)\n",
    "        self.bns = nn.Sequential(*self.bns)\n",
    "    \n",
    "        self.conv_width = nn.Conv2d(in_c, out_c, 1)\n",
    "        self.activation = nn.ELU(inplace=True)\n",
    "        \n",
    "        self.head = [nn.Dropout(p=dropout)]\n",
    "        if reduce:\n",
    "            self.head = [nn.MaxPool2d(kernel_size=2, stride=2)] + self.head\n",
    "            \n",
    "        self.head = nn.Sequential(*self.head)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for num, conv, bn in zip(range(len(self.convs)), self.convs, self.bns):\n",
    "            ident = self.conv_width(x) if num == 0 else x\n",
    "            x = bn(conv(x))\n",
    "            x = x + ident\n",
    "            self.activation(x)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class ClfModel(nn.Module):\n",
    "    def __init__(self, chan_mult=2, layers=[4, 4, 4], dropout=[0.15, 0.25, 0.35, 0.60]) -> None:\n",
    "        super().__init__()\n",
    "        c_size = np.array([32, 64, 128]) * chan_mult\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            CNNBlock(3, c_size[0], layers=layers[0], dropout=dropout[0]),  # 0.15\n",
    "            CNNBlock(c_size[0], c_size[1], layers=layers[1], dropout=dropout[1]), # 0.25\n",
    "            CNNBlock(c_size[1], c_size[2], layers=layers[2], dropout=dropout[2]) # 0.35\n",
    "        )\n",
    "        self.pt_wise_convs = nn.Sequential(\n",
    "            nn.Conv2d(3, c_size[0], 1, stride=2),\n",
    "            nn.Conv2d(c_size[0], c_size[1], 1, stride=2),\n",
    "            nn.Conv2d(c_size[1], c_size[2], 1, stride=2)\n",
    "        )\n",
    "            \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c_size[-1] * (NETWORK_SIZE[0] // 2**3)**2, 800),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(800, momentum=0.1),\n",
    "            nn.Dropout(p=dropout[3]), # 0.6\n",
    "            nn.Linear(800, LABELS_CNT),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block, pt_wise in zip(self.blocks, self.pt_wise_convs):\n",
    "            ident = x\n",
    "            x = block(x) + pt_wise(ident)\n",
    "            \n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-18T21:29:37.829827Z",
     "iopub.status.busy": "2025-01-18T21:29:37.829426Z",
     "iopub.status.idle": "2025-01-18T21:29:39.297364Z",
     "shell.execute_reply": "2025-01-18T21:29:39.296665Z",
     "shell.execute_reply.started": "2025-01-18T21:29:37.829790Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning rate</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train Loss</td><td>▁</td></tr><tr><td>Valid Accuracy</td><td>▁</td></tr><tr><td>Valid Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning rate</td><td>0.005</td></tr><tr><td>Train Accuracy</td><td>1.81588</td></tr><tr><td>Train Loss</td><td>5.39696</td></tr><tr><td>Valid Accuracy</td><td>6.25</td></tr><tr><td>Valid Loss</td><td>4.83066</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Good big</strong> at: <a href='https://wandb.ai/renedyn-hse/DL%20LHW%201/runs/laiweye8' target=\"_blank\">https://wandb.ai/renedyn-hse/DL%20LHW%201/runs/laiweye8</a><br> View project at: <a href='https://wandb.ai/renedyn-hse/DL%20LHW%201' target=\"_blank\">https://wandb.ai/renedyn-hse/DL%20LHW%201</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250118_212317-laiweye8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T08:59:00.008848Z",
     "iopub.status.busy": "2025-01-19T08:59:00.008551Z",
     "iopub.status.idle": "2025-01-19T08:59:00.130552Z",
     "shell.execute_reply": "2025-01-19T08:59:00.129614Z",
     "shell.execute_reply.started": "2025-01-19T08:59:00.008825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model_state = model.state_dict() \n",
    "model_state = torch.load('/kaggle/input/big-good/model-4.pt', DEVICE, weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T08:59:02.406119Z",
     "iopub.status.busy": "2025-01-19T08:59:02.405795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = train_detector(\n",
    "    train_gt, \n",
    "    '/kaggle/input/bhw-1-dl-2024-2025/bhw1/trainval',\n",
    "    run_name='Good big 2',\n",
    "    epoch_cnt=30, \n",
    "    lr=0.001,\n",
    "    cos_restart=10, \n",
    "    batch_size=64,\n",
    "    weight_decay=5e-4, \n",
    "    sgd_momentum=0.9,\n",
    "    chan_mult=5, \n",
    "    layers=[4, 7, 5],\n",
    "    dropout=[0.15, 0.25, 0.35, 0.6],\n",
    "    model_state=model_state#model_state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:35.533373Z",
     "iopub.status.busy": "2025-01-19T13:13:35.533086Z",
     "iopub.status.idle": "2025-01-19T13:13:35.539141Z",
     "shell.execute_reply": "2025-01-19T13:13:35.538278Z",
     "shell.execute_reply.started": "2025-01-19T13:13:35.533352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calc_ans(model_filename, test_img_dir):\n",
    "    model = ClfModel(5, [4, 7, 5], [0.15, 0.25, 0.35, 0.6]).to(DEVICE)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_filename, DEVICE, weights_only=True))\n",
    "    model = model.eval()\n",
    "    results = dict()\n",
    "    img_paths = glob.glob(f'{test_img_dir}/*')\n",
    "    progress_train = tqdm(\n",
    "        total=len(img_paths),\n",
    "        leave=False,\n",
    "    )\n",
    "    for img_path in img_paths:\n",
    "        filename = img_path.split('/')[-1]\n",
    "        image = torchvision.io.read_image(img_path)\n",
    "        image = DEFAULT_TRANSFORM(image).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            label = model(image[None, ...])[0].argmax().cpu()\n",
    "        results[filename] = int(label)\n",
    "        progress_train.update()\n",
    "    progress_train.close()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T13:13:48.115575Z",
     "iopub.status.busy": "2025-01-19T13:13:48.115136Z",
     "iopub.status.idle": "2025-01-19T13:16:03.893973Z",
     "shell.execute_reply": "2025-01-19T13:16:03.893278Z",
     "shell.execute_reply.started": "2025-01-19T13:13:48.115534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = calc_ans('/kaggle/input/weights-new/model_checkpoint_Good big 2_e20.pt', '/kaggle/input/bhw-1-dl-2024-2025/bhw1/test')\n",
    "pd.DataFrame({\n",
    "    \"Id\": answer.keys(),\n",
    "    \"Category\": answer.values()\n",
    "}).to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10379392,
     "sourceId": 89777,
     "sourceType": "competition"
    },
    {
     "datasetId": 6502599,
     "sourceId": 10503608,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6507820,
     "sourceId": 10513623,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6508866,
     "sourceId": 10515550,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
