{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks (GANs) are an approach to generative modeling based on deep learning methods.\n",
    "\n",
    "The standard problem settings for GANs are generation of photorealistic images and image-to-image translation tasks (translating photos of summer to winter, day to night etc.).\n",
    "\n",
    "In this task you will familiarize yourself with both these problems while trying to create fake images of sneakers. \n",
    "\n",
    "As GANs still do have certain limitations about generating large images, the task is decomposed into two: first, use a simple GAN to generate a bunch of low resolution images from noise, then upscale them using another generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is built around [PyTorch](https://pytorch.org/) and [Lightning](https://pytorchlightning.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -d data/ ]; then\n",
    "    curl -sO 'https://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip'\n",
    "    unzip -qo data.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = (28, 28)\n",
    "NOISE_DIM = 100\n",
    "LOW_RES_SIZE = IMAGE_SIZE\n",
    "HIGH_RES_SIZE = (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5729"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_kwargs = {}\n",
    "dataset_root = \"./data\"\n",
    "images_dir = \"images\"\n",
    "image_filenames = sorted(os.listdir(os.path.join(dataset_root, images_dir)))\n",
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Image Generation\n",
    "\n",
    "Your first task is to solve a problem of generating photorealistic images out of noise (okay, this might sound optimistic).\n",
    "\n",
    "Namely, you are required to **create fake images of sneakers of resolution 28x28 given a vector of noise sampled from standard normal distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is simple PyTorch `Dataset` class for loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SneakersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        images_dir=\"images\",\n",
    "        input_size=None,\n",
    "        target_size=None,\n",
    "    ):\n",
    "        self.images_dir = os.path.join(root_dir, images_dir)\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        files = os.listdir(self.images_dir)\n",
    "        self.all_images = sorted([file for file in files if file.endswith(\".jpg\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.all_images[idx])\n",
    "        image_bgr = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        if self.input_size is not None:\n",
    "            input_rgb = cv2.resize(image_rgb, self.input_size)\n",
    "            input_rgb = (input_rgb / 255).astype(np.float32)\n",
    "        if self.target_size is not None:\n",
    "            image_rgb = cv2.resize(image_rgb, self.target_size)\n",
    "        image_rgb = (image_rgb / 255).astype(np.float32)\n",
    "\n",
    "        # tanh values in [-1, 1]\n",
    "        image_rgb = torch.from_numpy(image_rgb * 2 - 1)\n",
    "        if self.input_size is not None:\n",
    "            input_rgb = torch.from_numpy(input_rgb * 2 - 1)\n",
    "            return input_rgb, image_rgb\n",
    "        else:\n",
    "            return image_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create `LightningDataModule` which will handle dataset loading in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SneakersGANDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = SneakersDataset(self.data_dir, target_size=IMAGE_SIZE)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2img(d: np.ndarray):\n",
    "    return 0.5 * d + 0.5\n",
    "\n",
    "\n",
    "def visualize_images(data, n_rows, n_cols):\n",
    "    n_samples = n_rows * n_cols\n",
    "\n",
    "    if len(data) != n_samples:\n",
    "        sample_indices = np.random.choice(\n",
    "            len(data),\n",
    "            n_samples,\n",
    "            replace=len(data) < n_samples,\n",
    "        )\n",
    "    else:\n",
    "        sample_indices = np.arange(len(data)).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(int(2.5 * n_cols), int(2.5 * n_rows)))\n",
    "    for i, sample_index in enumerate(sample_indices):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(data2img(data[sample_index]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=32)\n",
    "dm.setup()\n",
    "visualize_images(next(iter(dm.train_dataloader())), 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (0.5 pts) [cross-check:0]\n",
    "\n",
    "Our first step is to build a generator. You should use the layers in `torch.nn` (imported in the beginning as `nn`) to construct the model. Since we are using PyTorch and Lightning, you should create `nn.Module`. Use the default initializers for parameters.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected (`Linear` in PyTorch) with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 28 x 28 x 3\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
    " * Reshape into (28, 28, 3)\n",
    "\n",
    "> You could perform reshaping inside of `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # TODO: add other layers\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 28 * 28 * 3),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z).reshape((-1, 28, 28, 3))\n",
    "        # keep batch size\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_gen = FCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
    "img_generated = fc_gen(torch.randn(32, NOISE_DIM))\n",
    "assert img_generated.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator (0.5 pts) [cross-check:1]\n",
    "\n",
    "Now you are to build a discriminator. You should use the default initializers for parameters here as well.\n",
    "\n",
    "Architecture:\n",
    " * Flatten\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 1\n",
    " * Sigmoid (to obtain logits as an output)\n",
    "\n",
    "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the probability that each of the `batch_size` inputs is a real image.\n",
    "\n",
    "> You could perform flattening inside of `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDiscriminator(nn.Module):\n",
    "    def __init__(self, img_shape: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            nn.Linear(28 * 28 * 3, 256), \n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256, 256), \n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        return self.model(img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_dis = FCDiscriminator(IMAGE_SIZE + (3,))\n",
    "prob_dis = fc_dis(img_generated)\n",
    "assert prob_dis.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss (1.0 pts) [cross-check:2]\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "Instead of computing the expectation, you may average over elements of the minibatch, so make sure to combine the loss by *averaging* instead of summing.\n",
    "\n",
    "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a `LightningModule` with all necessary methods. You should fill all `TODO` comments.\n",
    "\n",
    "> Don't forget to use `detach` method for discriminator training step to prevent backpropagation for generator compuational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGAN(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        channels,\n",
    "        noise_dim=100,\n",
    "        lr=0.0002,\n",
    "        b1=0.5,\n",
    "        b2=0.999,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # make <arg> available as self.hparams.<arg>\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        img_shape = (width, height, channels)\n",
    "        self.generator = FCGenerator(\n",
    "            noise_dim=self.hparams.noise_dim,\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "        self.discriminator = FCDiscriminator(\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "\n",
    "        self.validation_z = torch.randn(8, noise_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def gan_loss(self, y_hat, y):\n",
    "        # TODO: implement GAN loss (hint: binary cross entropy)\n",
    "    \n",
    "        return -(y * torch.log(y_hat) + (1-y) * torch.log(1-y_hat)).mean()\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
    "        # move to same device as imgs\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # optimize generator\n",
    "        self.generated_imgs = self(z)\n",
    "\n",
    "        # all fake, but we want to be real\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        g_loss = self.gan_loss(self.discriminator(self(z)), valid)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "        # TODO: loss for `discriminator(imgs)` and `valid`\n",
    "        real_loss = ...\n",
    "\n",
    "        # TODO: zero vector for fake_loss computation\n",
    "        fake = torch.zeros(imgs.size(0))\n",
    "        fake = fake.type_as(imgs)\n",
    "\n",
    "        fake_loss = self.gan_loss(self.discriminator(self(z).detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # send results for validation_z to TensorBoard\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type            | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | generator     | FCGenerator     | 3.6 M  | train\n",
      "1 | discriminator | FCDiscriminator | 668 K  | train\n",
      "----------------------------------------------------------\n",
      "4.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 M     Total params\n",
      "16.929    Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/45 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/45 [00:00<?, ?it/s, v_num=1, g_loss=0.682, d_loss=0.703]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mSneakersDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m image_bgr \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 21\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m FCGAN(\u001b[38;5;241m*\u001b[39mIMAGE_SIZE, \u001b[38;5;241m3\u001b[39m, NOISE_DIM)\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "model = FCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
    "trainer = L.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(model(torch.randn(32, NOISE_DIM)).detach(), 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hint: this architecture isn't great, so don't spend all your time to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional GANs\n",
    "In the first part of the notebook, you have implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, you are to implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where both discriminator and generator are convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (1.0 pts) [cross-check:3]\n",
    "\n",
    "Architecture:\n",
    " * Fully connected with output size of 128x7x7\n",
    " * Reshape into (128, 7, 7)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=128, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=64, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * ReLU\n",
    " * Conv2D: 3x3, filters=3, padding=\"same\"\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
    "\n",
    "> There is no `padding=\"same\"` in PyTorch, but you could use `padding=k//2` and `padding_mode=\"zeros\"` for kernel size `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        # make channel axis last\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_gen = DCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
    "fake_imgs = dc_gen(torch.randn(10, NOISE_DIM))\n",
    "assert fake_imgs.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator (1.0 pts) [cross-check:4]\n",
    "\n",
    "Architecture:\n",
    " * Conv2D: 3x3, filters=32, strides=2, padding=\"same\"\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=64, strides=2, padding=\"same\"\n",
    " * Zero Padding 2D: ((0, 1), (0, 1))\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=128, strides=2, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=256, strides=2, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Flatten\n",
    " * Fully connected layer with output size 1\n",
    " * Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, img_shape: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # channels first\n",
    "        img_chan = img.permute(0, 3, 1, 2)\n",
    "        return self.model(img_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_dis = DCDiscriminator(img_shape=IMAGE_SIZE + (3,))\n",
    "fake_proba = dc_dis(fake_imgs)\n",
    "assert fake_proba.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares GAN loss (1.0 pts) [cross-check:5]\n",
    "We'll now look at [Least Squares GAN loss](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all you have to do is change the loss function and retrain the model. You'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "and the discriminator loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        channels,\n",
    "        noise_dim=100,\n",
    "        lr=0.0002,\n",
    "        b1=0.5,\n",
    "        b2=0.999,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # make <arg> available as self.hparams.<arg>\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        img_shape = (width, height, channels)\n",
    "        self.generator = DCGenerator(\n",
    "            noise_dim=self.hparams.noise_dim,\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "        self.discriminator = DCDiscriminator(\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "\n",
    "        self.validation_z = torch.randn(8, noise_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def ls_loss(self, y_hat, y):\n",
    "        # TODO: implement least squares GAN loss\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
    "        # move to same device as imgs\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # optimize generator\n",
    "        # TODO: compute loss as before (FCGAN)\n",
    "        ...\n",
    "\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        # TODO: compute loss as before (FCGAN)\n",
    "        ...\n",
    "\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Train generator and discriminator in a loop and draw results once every N iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "model = DCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
    "trainer = L.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for **III. GAN metrics. PRD score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: L.LightningModule,\n",
    "    data: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    output = []\n",
    "    slicer = range(0, len(data), batch_size)\n",
    "    if verbose:\n",
    "        slicer = tqdm.tqdm(slicer)\n",
    "\n",
    "    model.eval()\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in slicer:\n",
    "            x = data[i : i + batch_size]\n",
    "            if use_gpu:\n",
    "                x = x.cuda()\n",
    "            y = model(x)\n",
    "            if use_gpu:\n",
    "                y = y.cpu()\n",
    "            output.append(y)\n",
    "    if use_gpu:\n",
    "        model.cpu()\n",
    "\n",
    "    output = torch.cat(output, dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SneakersDataset(\"data\", target_size=IMAGE_SIZE)\n",
    "lr_data = torch.stack([dataset[i] for i in range(len(dataset))], dim=0)\n",
    "dc_fake_data = predict(model, torch.randn(lr_data.size(0), NOISE_DIM), BATCH_SIZE)\n",
    "print(\"LR data:\", lr_data.size())\n",
    "print(\"Fake LR data:\", dc_fake_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(dc_fake_data, 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Super Resolution\n",
    "\n",
    "In this part of the notebook you will train a generative model that solves an image-to-image problem, with \"small images\" as a source domain and \"large images\" being a target domain. \n",
    "\n",
    "To specify the task, you are to **scale small images of 28x28 pixels up to size of 112x112 pixels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SneakersSRDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = SneakersDataset(\n",
    "            self.data_dir,\n",
    "            input_size=LOW_RES_SIZE,\n",
    "            target_size=HIGH_RES_SIZE,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dm = SneakersSRDataModule(\"data\", batch_size=batch_size)\n",
    "dm.setup()\n",
    "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
    "\n",
    "assert real_data_lr.shape[1:] == LOW_RES_SIZE + (3,)\n",
    "assert real_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)\n",
    "\n",
    "plt.figure(figsize=(8, 14))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(batch_size, 2, 2 * i + 1)\n",
    "    plt.title(\"Low resolution\")\n",
    "    plt.imshow(data2img(real_data_lr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(batch_size, 2, 2 * i + 2)\n",
    "    plt.title(\"High resolution\")\n",
    "    plt.imshow(data2img(real_data_hr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this task, you are to train an [SRGAN](https://arxiv.org/abs/1609.04802)-like model. For your convinience, some layers are already implemented. Now it's your turn -- fill the gaps so the model passes the asserts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator (1.5 pts) [cross-check:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a SRGAN Generator, you will need a basic Residual Block(filters):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU\n",
    "* Batch Normalization 2D with momentum(0.8)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* Batch Normalization 2D with momentum(0.8)\n",
    "* Sum up outputs with inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z) + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsampling Block(filters):\n",
    "\n",
    "* UpSampling2D(2)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, filters: int, input_filters: int = None):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these basic building blocks, one is able to define a SRGAN generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.residual_chain = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(16)],\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=3 // 2, padding_mode=\"zeros\"),\n",
    "            nn.BatchNorm2d(64, momentum=0.8)\n",
    "        )\n",
    "        self.upsample_conv = nn.Sequential(\n",
    "            UpsamplingBlock(256, input_filters=64),\n",
    "            UpsamplingBlock(256),\n",
    "            nn.Conv2d(256, 3, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        conv = self.init_conv(x)\n",
    "        x = self.residual_chain(conv) + conv\n",
    "        img = self.upsample_conv(x)\n",
    "        # make channel axis last\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
    "fake_hr = SRGenerator()(real_data_lr)\n",
    "assert fake_hr.shape == real_data_hr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator (1.5 pts) [cross-check:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a Discriminator Block(filters, strides):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=strides, padding=\"same\"\n",
    "* Leaky ReLU(0.2)\n",
    "* (optional) Batch Normalization 2D with momentum(0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_filters: int,\n",
    "        filters: int,\n",
    "        strides: int,\n",
    "        batch_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            # TODO: add layers (except BatchNorm2d)\n",
    "            ...\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(filters, momentum=0.8))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this block to build up SRGAN discriminator:\n",
    "\n",
    "* DBlock(filters=64, strides=1) with Batch Normalization\n",
    "* DBlock(filters=64, strides=2)\n",
    "* DBlock(filters=128, strides=1)\n",
    "* DBlock(filters=128, strides=2)\n",
    "* DBlock(filters=256, strides=1)\n",
    "* DBlock(filters=256, strides=2)\n",
    "* DBlock(filters=512, strides=1)\n",
    "* DBlock(filters=512, strides=2)\n",
    "* Flatten\n",
    "* Fully connected with output size of 1024\n",
    "* Leaky ReLU(0.2)\n",
    "* Fully connected with output size of 1\n",
    "* Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_probas = SRDiscriminator()(fake_hr)\n",
    "assert fake_probas.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, SRGAN is trained with additional loss on features from pretrained VGG-19. However, you task will be a bit simplier: use **mean squared error** between real and fake data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN(L.LightningModule):\n",
    "    def __init__(self, validation_lr):\n",
    "        super().__init__()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.generator = SRGenerator()\n",
    "        self.discriminator = SRDiscriminator()\n",
    "\n",
    "        self.validation_lr = validation_lr\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def gan_loss(self, y_hat, y):\n",
    "        # TODO: implement loss\n",
    "        return ...\n",
    "\n",
    "    def ls_loss(self, y_hat, y):\n",
    "        # TODO: implement loss\n",
    "        return ...\n",
    "\n",
    "    def mse_loss(self, sr_imgs, hr_imgs):\n",
    "        # TODO: implement MSE loss\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        lr_imgs, hr_imgs = imgs\n",
    "\n",
    "        # optimize generator\n",
    "        sr_imgs = self(lr_imgs)\n",
    "\n",
    "        # all fake, but we want to be real\n",
    "        valid = torch.ones(lr_imgs.size(0), 1)\n",
    "        valid = valid.type_as(lr_imgs)\n",
    "\n",
    "        gan_loss = self.gan_loss(self.discriminator(sr_imgs), valid)\n",
    "        mse_loss = self.mse_loss(sr_imgs, hr_imgs)\n",
    "\n",
    "        g_loss = 1 * mse_loss + 0.5 * gan_loss\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.log(\"gan_loss\", gan_loss, prog_bar=True)\n",
    "        self.log(\"mse_loss\", mse_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        valid = torch.ones(hr_imgs.size(0), 1)\n",
    "        valid = valid.type_as(hr_imgs)\n",
    "        real_loss = self.ls_loss(self.discriminator(hr_imgs), valid)\n",
    "\n",
    "        fake = torch.zeros(lr_imgs.size(0), 1)\n",
    "        fake = fake.type_as(lr_imgs)\n",
    "        fake_loss = self.ls_loss(self.discriminator(self(lr_imgs).detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=2e-4,\n",
    "            betas=(0.5, 0.999),\n",
    "        )\n",
    "        opt_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.5, 0.999),\n",
    "        )\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_lr.type_as(self.generator.init_conv[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SneakersSRDataModule(\"data\", batch_size=16)\n",
    "dm.setup()\n",
    "model = SRGAN(validation_lr=next(iter(dm.train_dataloader()))[0][:4])\n",
    "trainer = L.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for **III. GAN metrics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = SneakersSRDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "dm.setup()\n",
    "hr_data = []\n",
    "for lr, hr in dm.train_dataloader():\n",
    "    hr_data.append(hr)\n",
    "hr_data = torch.cat(hr_data, dim=0)\n",
    "\n",
    "batch_size = 16\n",
    "sr_fake_data_from_real_lr = predict(model, lr_data, batch_size)\n",
    "sr_fake_data_from_fake_lr = predict(model, dc_fake_data, batch_size)\n",
    "print(\"HR data:\", hr_data.size())\n",
    "print(\"SR from real LR:\", sr_fake_data_from_real_lr.size())\n",
    "print(\"SR from fake LR:\", sr_fake_data_from_fake_lr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(sr_fake_data_from_real_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(sr_fake_data_from_fake_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. GAN metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a few metrics used to measure GAN performance. Some of them are based on comparing real samples against generated ones, while the other rely on additional pretrained models that are applied to both real and generated data in order to accumulate high-level statistics. In this task, you are going to use two metrics representing these two approaches -- namely, [Precision-Recall Density](https://arxiv.org/pdf/1806.00035) and [Fréchet Inception Distance](https://arxiv.org/pdf/1706.08500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Precision-Recall Density (PRD score) (1.0 pts) [cross-check:8]\n",
    "\n",
    "Your first task is to implement [Precision-Recall Density](https://arxiv.org/pdf/1806.00035.pdf) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "\n",
    "def bin_counts(real_data, fake_data, n_bins=25):\n",
    "    real_data = real_data.reshape(len(real_data), -1)\n",
    "    fake_data = fake_data.reshape(len(fake_data), -1)\n",
    "\n",
    "    data = np.vstack([real_data, fake_data])\n",
    "\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_bins, n_init=10).fit(data)\n",
    "\n",
    "    real_labels = kmeans.labels_[: len(real_data)]\n",
    "    fake_labels = kmeans.labels_[len(real_data) :]\n",
    "\n",
    "    real_density, _ = np.histogram(\n",
    "        real_labels,\n",
    "        bins=n_bins,\n",
    "        range=[0, n_bins],\n",
    "        density=True,\n",
    "    )\n",
    "    # TODO: same for fake_labels\n",
    "    fake_density, _ = np.histogram(\n",
    "        fake_labels,\n",
    "        bins=n_bins,\n",
    "        range=[0, n_bins],\n",
    "        density=True,\n",
    "    )\n",
    "\n",
    "    return real_density, fake_density\n",
    "\n",
    "\n",
    "def sample_bin_counts(real_data, fake_data, n_bins=25, repeat_number=10, verbose=True):\n",
    "    real_densities = []\n",
    "    fake_densities = []\n",
    "    counter = range(repeat_number)\n",
    "    if verbose:\n",
    "        counter = tqdm.tqdm(counter)\n",
    "    for _ in counter:\n",
    "        real, fake = bin_counts(real_data, fake_data, n_bins=n_bins)\n",
    "        real_densities.append(real)\n",
    "        fake_densities.append(fake)\n",
    "    return np.array(real_densities).mean(axis=0), np.array(fake_densities).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def calculate_alpha_beta(real_density, fake_density, n_thetas=1000):\n",
    "    assert real_density.shape == fake_density.shape\n",
    "\n",
    "    alpha = []\n",
    "    beta = []\n",
    "\n",
    "    thetas = np.linspace(1e-6, np.pi / 2 - 1e-6, num=n_thetas)\n",
    "    for theta in thetas:\n",
    "        tan = math.tan(theta)\n",
    "        # TODO: implement paper formula\n",
    "        alpha.append(np.minimum(tan * real_density, fake_density).sum())\n",
    "        beta.append(alpha[-1] / tan)\n",
    "\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "def calculate_prd_score(real_data, fake_data):\n",
    "    # Calculate bin counts from real and generated data multiple times\n",
    "    # TODO\n",
    "    real_density, fake_density = sample_bin_counts(real_data, fake_data)\n",
    "\n",
    "    plt.bar(\n",
    "        range(len(real_density)),\n",
    "        real_density,\n",
    "        width=1,\n",
    "        color=\"g\",\n",
    "        alpha=0.5,\n",
    "        label=\"Real density\",\n",
    "    )\n",
    "    plt.bar(\n",
    "        range(len(fake_density)),\n",
    "        fake_density,\n",
    "        width=1,\n",
    "        color=\"r\",\n",
    "        alpha=0.5,\n",
    "        label=\"Fake density\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate alpha and beta\n",
    "    # TODO\n",
    "    alpha, beta = calculate_alpha_beta(real_density, fake_density)\n",
    "\n",
    "    # Calculate area under curve (AUC) for alpha and beta\n",
    "    # TODO\n",
    "    score = auc(alpha, beta)\n",
    "\n",
    "    return score, alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate PRD score for DCGAN (task I). You should pass `lr_data` and `dc_fake_data` to scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _, _ = calculate_prd_score(\n",
    "    lr_data,\n",
    "    dc_fake_data,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use PRD score to compare high resolution data generated from real low resolution data (`sr_fake_data_from_real_lr`) and fake resolution data (`sr_fake_data_from_fake_lr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated from real LR\")\n",
    "score_real, alpha_real, beta_real = calculate_prd_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_real_lr,\n",
    ")\n",
    "print(\"Score:\", score_real, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Generated from fake LR\")\n",
    "score_fake, alpha_fake, beta_fake = calculate_prd_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_fake_lr,\n",
    ")\n",
    "print(\"Score:\", score_fake, end=\"\\n\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(alpha_real, beta_real, color=\"g\", label=\"Generated from real LR\")\n",
    "plt.plot(alpha_fake, beta_fake, color=\"r\", label=\"Generated from fake LR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet Inception Distance (FID score) (1.0 pts) [cross-check:9]\n",
    "\n",
    "[Frechet Inception Distance](https://arxiv.org/pdf/1706.08500) is an improved version of [Inception score](https://arxiv.org/abs/1606.03498), that additionally calculates the statistics of real data and compares it to the statistics of generated data. It is probably the most widely-used option for evaluating GANs, and relies on features extracted with [InceptionV3](https://arxiv.org/abs/1512.00567) pretrained on ImageNet. These features assumed to come from a multivariate Gaussian distribution, so Fréchet distance between two multivariate Gaussians can be calculated:\n",
    "\n",
    "$$\\text{FID} = ||\\mu_r - \\mu_g||^2 + \\text{Tr} (\\Sigma_r + \\Sigma_g - 2 (\\Sigma_r \\Sigma_g)^{1/2}),$$\n",
    "\n",
    "where $X_r \\sim \\mathcal{N} (\\mu_r, \\Sigma_r)$ and $X_g \\sim \\mathcal{N} (\\mu_g, \\Sigma_g)$ are the 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create InceptionV3 ([from torch repository](https://pytorch.org/hub/pytorch_vision_inception_v3/)) model. As you will be using it for feature extraction only, we should remove last fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionHeadless(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.model = torchvision.models.inception_v3(\n",
    "            weights=torchvision.models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "        )\n",
    "        # remove last fc layer\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        x = self.upsample(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception = InceptionHeadless()\n",
    "inception.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    assert inception(hr_data[:32]).shape[1:] == (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import linalg\n",
    "\n",
    "def calculate_activations(data, batch_size=32, verbose=False):\n",
    "    # Calculate activations of Pool3 layer of InceptionV3\n",
    "    if verbose:\n",
    "        print(\"Calculating activations...\")\n",
    "    activations = predict(inception, data, batch_size=32).detach().numpy()\n",
    "    return activations\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(activations):\n",
    "    # Calculate mean and covariance of activations. Mind the dimensions!\n",
    "    # TODO\n",
    "    mu = activations.mean(axis=0)\n",
    "    t = activations - mu\n",
    "    # TODO\n",
    "    sigma = (t[:, :, None] @ t[:, None, :]).sum(axis=0)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_activations = calculate_activations(hr_data, verbose=True)\n",
    "real_mu, real_sigma = calculate_activation_statistics(torch.zeros(32, 2048))\n",
    "\n",
    "assert real_mu.shape == (2048,)\n",
    "assert real_sigma.shape == (2048, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    assert mu1.shape == mu2.shape\n",
    "    assert sigma1.shape == sigma2.shape\n",
    "\n",
    "    sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1, sigma2))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(sigma1_sigma2):\n",
    "        sigma1_sigma2 = sigma1_sigma2.real\n",
    "\n",
    "    # Product might be almost singular\n",
    "    if not np.isfinite(sigma1_sigma2).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1 + offset, sigma2 + offset))\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # use diff, sigma1, sigma2 to calculate FID according to the formula above\n",
    "    # TODO: implement score from paper\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(sigma1_sigma2)\n",
    "\n",
    "\n",
    "def calculate_fid_score(real_data, fake_data, verbose=False):\n",
    "    # Run inception on real and fake data to obtain activations\n",
    "    # TODO\n",
    "    real_activations = calculate_activations(real_data)\n",
    "    fake_activations = calculate_activations(fake_data)\n",
    "\n",
    "    # Calculate mu and sigma for both real and fake activations\n",
    "    # TODO\n",
    "    real_mu, real_sigma = calculate_activation_statistics(real_activations)\n",
    "    fake_mu, fake_sigma = calculate_activation_statistics(fake_activations)\n",
    "\n",
    "    # Calculate Frechet distance\n",
    "    return calculate_frechet_distance(\n",
    "        real_mu,\n",
    "        real_sigma,\n",
    "        fake_mu,\n",
    "        fake_sigma,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate FID score between `hr_data` and `sr_fake_data_from_real_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_fid_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_real_lr,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together: calculate FID score between `hr_data` and `sr_fake_data_from_fake_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = calculate_fid_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_fake_lr,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
